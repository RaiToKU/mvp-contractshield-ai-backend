import os
import requests
from typing import List, Dict, Any
from sqlalchemy.orm import Session
from sqlalchemy import text
import logging
import json

from ..models import Paragraph, Risk, Statute
from ..database import SessionLocal

logger = logging.getLogger(__name__)

class AIService:
    """AIæœåŠ¡ï¼Œå¤„ç†OpenRouter APIè°ƒç”¨å’Œå‘é‡æ£€ç´¢"""
    
    def __init__(self):
        self.api_key = os.getenv("OPENROUTER_API_KEY", "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")
        self.base_url = "https://openrouter.ai/api/v1/chat/completions"
        self.embedding_model = "text-embedding-ada-002"  # ä¿ç•™ç”¨äºå‘é‡åŒ–
        self.chat_model = "qwen/qwen3-235b-a22b:free"
    
    def _call_openrouter_api(self, messages: List[Dict], temperature: float = 0.1) -> str:
        """è°ƒç”¨OpenRouter API"""
        try:
            response = requests.post(
                url=self.base_url,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                },
                data=json.dumps({
                    "model": self.chat_model,
                    "messages": messages,
                    "temperature": temperature
                })
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content']
            else:
                logger.error(f"OpenRouter API error: {response.status_code} - {response.text}")
                raise Exception(f"API call failed: {response.status_code}")
                
        except Exception as e:
            logger.error(f"Error calling OpenRouter API: {e}")
            raise
    
    async def get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤ºï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨æ–‡æœ¬hashä½œä¸ºå‘é‡ï¼‰"""
        try:
            # ç®€åŒ–ç‰ˆå‘é‡åŒ–ï¼šä½¿ç”¨æ–‡æœ¬çš„hashå€¼ç”Ÿæˆå›ºå®šé•¿åº¦å‘é‡
            import hashlib
            text_hash = hashlib.md5(text.encode()).hexdigest()
            # å°†hashè½¬æ¢ä¸º1536ç»´å‘é‡ï¼ˆæ¨¡æ‹ŸOpenAI embeddingç»´åº¦ï¼‰
            vector = []
            for i in range(0, len(text_hash), 2):
                vector.append(int(text_hash[i:i+2], 16) / 255.0)
            # è¡¥é½åˆ°1536ç»´
            while len(vector) < 1536:
                vector.extend(vector[:min(len(vector), 1536-len(vector))])
            return vector[:1536]
        except Exception as e:
            logger.error(f"Error getting embedding: {e}")
            raise
    
    def get_embedding_sync(self, text: str) -> List[float]:
        """åŒæ­¥è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º"""
        try:
            # ç®€åŒ–ç‰ˆå‘é‡åŒ–ï¼šä½¿ç”¨æ–‡æœ¬çš„hashå€¼ç”Ÿæˆå›ºå®šé•¿åº¦å‘é‡
            import hashlib
            text_hash = hashlib.md5(text.encode()).hexdigest()
            # å°†hashè½¬æ¢ä¸º1536ç»´å‘é‡ï¼ˆæ¨¡æ‹ŸOpenAI embeddingç»´åº¦ï¼‰
            vector = []
            for i in range(0, len(text_hash), 2):
                vector.append(int(text_hash[i:i+2], 16) / 255.0)
            # è¡¥é½åˆ°1536ç»´
            while len(vector) < 1536:
                vector.extend(vector[:min(len(vector), 1536-len(vector))])
            return vector[:1536]
        except Exception as e:
            logger.error(f"Error getting embedding: {e}")
            raise
    
    def vectorize_paragraphs(self, task_id: int, paragraphs: List[str]):
        """å¯¹æ®µè½è¿›è¡Œå‘é‡åŒ–å¹¶å­˜å‚¨"""
        db = SessionLocal()
        try:
            for i, para_text in enumerate(paragraphs):
                # è·å–å‘é‡
                embedding = self.get_embedding_sync(para_text)
                
                # å­˜å‚¨æ®µè½å’Œå‘é‡
                paragraph = Paragraph(
                    task_id=task_id,
                    text=para_text,
                    embedding=embedding,
                    paragraph_index=i
                )
                db.add(paragraph)
            
            db.commit()
            logger.info(f"Vectorized {len(paragraphs)} paragraphs for task {task_id}")
            
        except Exception as e:
            db.rollback()
            logger.error(f"Error vectorizing paragraphs: {e}")
            raise
        finally:
            db.close()
    
    def search_similar_paragraphs(self, query_text: str, task_id: int, limit: int = 5) -> List[Dict]:
        """åŸºäºå‘é‡ç›¸ä¼¼åº¦æœç´¢ç›¸å…³æ®µè½"""
        db = SessionLocal()
        try:
            # è·å–æŸ¥è¯¢å‘é‡
            query_embedding = self.get_embedding_sync(query_text)
            
            # ä½¿ç”¨PGVectorè¿›è¡Œç›¸ä¼¼åº¦æœç´¢
            sql = text("""
                SELECT id, text, paragraph_index,
                       embedding <-> :query_embedding as distance
                FROM paragraphs 
                WHERE task_id = :task_id
                ORDER BY embedding <-> :query_embedding
                LIMIT :limit
            """)
            
            result = db.execute(sql, {
                'query_embedding': str(query_embedding),
                'task_id': task_id,
                'limit': limit
            })
            
            similar_paragraphs = []
            for row in result:
                similar_paragraphs.append({
                    'id': row.id,
                    'text': row.text,
                    'paragraph_index': row.paragraph_index,
                    'similarity_score': 1 - row.distance  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                })
            
            return similar_paragraphs
            
        except Exception as e:
            logger.error(f"Error searching similar paragraphs: {e}")
            return []
        finally:
            db.close()
    
    def extract_entities_ner(self, text: str) -> Dict[str, List[str]]:
        """ä½¿ç”¨NERæå–å®ä½“ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…å¯ç”¨spaCyç­‰ï¼‰"""
        logger.info(f"ğŸ” Starting entity extraction for text length: {len(text)}")
        
        try:
            # é¢„å¤„ç†æ–‡æœ¬ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹
            if len(text.strip()) < 10:
                logger.warning("âš ï¸ Text too short for entity extraction")
                return self._get_fallback_entities(text)
            
            prompt = f"""
            è¯·ä»”ç»†åˆ†æä»¥ä¸‹åˆåŒæ–‡æœ¬ï¼Œæå–æ‰€æœ‰å¯èƒ½çš„å½“äº‹æ–¹åç§°ã€‚è¯·ç‰¹åˆ«æ³¨æ„ï¼š
            1. å…¬å¸åç§°ï¼ˆåŒ…å«"æœ‰é™å…¬å¸"ã€"è‚¡ä»½å…¬å¸"ã€"é›†å›¢"ç­‰ï¼‰
            2. ä¸ªäººå§“åï¼ˆé€šå¸¸åœ¨ç”²æ–¹ã€ä¹™æ–¹ã€å§”æ‰˜æ–¹ã€å—æ‰˜æ–¹ç­‰ä½ç½®ï¼‰
            3. å…¶ä»–ç»„ç»‡æœºæ„åç§°
            
            åˆåŒæ–‡æœ¬ï¼š
            {text[:3000]}
            
            è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š
            {{
                "companies": ["å…¬å¸åç§°1", "å…¬å¸åç§°2"],
                "persons": ["å§“å1", "å§“å2"],
                "organizations": ["ç»„ç»‡åç§°1", "ç»„ç»‡åç§°2"]
            }}
            """
            
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åˆåŒå®ä½“æå–ä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†ææ–‡æœ¬å¹¶å‡†ç¡®æå–æ‰€æœ‰å½“äº‹æ–¹ä¿¡æ¯ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            result_text = self._call_openrouter_api(messages, temperature=0.1)
            logger.info(f"ğŸ¤– AI response: {result_text[:200]}...")
            
            # å°è¯•è§£æJSON
            entities = self._parse_entities_response(result_text)
            
            # å¦‚æœAIæå–å¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¤‡ç”¨æ–¹æ¡ˆ
            if not any(entities.values()):
                logger.warning("âš ï¸ AI extraction returned empty, trying regex fallback")
                entities = self._extract_entities_regex(text)
            
            logger.info(f"âœ… Final entities extracted: {entities}")
            return entities
                
        except Exception as e:
            logger.error(f"âŒ Error in NER extraction: {e}")
            return self._extract_entities_regex(text)
    
    def _parse_entities_response(self, result_text: str) -> Dict[str, List[str]]:
        """è§£æAIè¿”å›çš„å®ä½“æå–ç»“æœ"""
        try:
            # ç›´æ¥è§£æJSON
            entities = json.loads(result_text)
            # éªŒè¯æ ¼å¼
            if isinstance(entities, dict) and all(key in entities for key in ["companies", "persons", "organizations"]):
                return entities
        except json.JSONDecodeError:
            pass
        
        # å°è¯•æå–JSONéƒ¨åˆ†
        import re
        json_match = re.search(r'\{[^{}]*"companies"[^{}]*\}', result_text, re.DOTALL)
        if json_match:
            try:
                entities = json.loads(json_match.group())
                return entities
            except json.JSONDecodeError:
                pass
        
        logger.warning("Failed to parse entities response as JSON")
        return {"companies": [], "persons": [], "organizations": []}
    
    def _extract_entities_regex(self, text: str) -> Dict[str, List[str]]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å®ä½“ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        logger.info("ğŸ”§ Using regex fallback for entity extraction")
        
        import re
        companies = []
        persons = []
        organizations = []
        
        # æå–å…¬å¸åç§°
        company_patterns = [
            r'([\u4e00-\u9fa5]+(?:æœ‰é™å…¬å¸|è‚¡ä»½æœ‰é™å…¬å¸|é›†å›¢|å…¬å¸))',
            r'(\w+(?:æœ‰é™å…¬å¸|è‚¡ä»½æœ‰é™å…¬å¸|é›†å›¢|å…¬å¸))',
        ]
        
        for pattern in company_patterns:
            matches = re.findall(pattern, text)
            companies.extend(matches)
        
        # æå–å¸¸è§çš„åˆåŒè§’è‰²æ ‡è¯†åçš„åç§°
        role_patterns = [
            r'ç”²æ–¹[ï¼š:](\s*[\u4e00-\u9fa5]+(?:æœ‰é™å…¬å¸|å…¬å¸|é›†å›¢)?)',
            r'ä¹™æ–¹[ï¼š:](\s*[\u4e00-\u9fa5]+(?:æœ‰é™å…¬å¸|å…¬å¸|é›†å›¢)?)',
            r'å§”æ‰˜æ–¹[ï¼š:](\s*[\u4e00-\u9fa5]+(?:æœ‰é™å…¬å¸|å…¬å¸|é›†å›¢)?)',
            r'å—æ‰˜æ–¹[ï¼š:](\s*[\u4e00-\u9fa5]+(?:æœ‰é™å…¬å¸|å…¬å¸|é›†å›¢)?)',
        ]
        
        for pattern in role_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                name = match.strip()
                if 'å…¬å¸' in name or 'é›†å›¢' in name:
                    companies.append(name)
                else:
                    persons.append(name)
        
        # å»é‡
        companies = list(set(companies))
        persons = list(set(persons))
        organizations = list(set(organizations))
        
        result = {
            "companies": companies,
            "persons": persons,
            "organizations": organizations
        }
        
        logger.info(f"ğŸ¯ Regex extraction result: {result}")
        return result
    
    def _get_fallback_entities(self, text: str) -> Dict[str, List[str]]:
        """è·å–å¤‡ç”¨å®ä½“ï¼ˆå½“æ–‡æœ¬å¤ªçŸ­æ—¶ï¼‰"""
        logger.info("ğŸ“ Using fallback entities for short text")
        return {
            "companies": ["åˆåŒå½“äº‹æ–¹"],
            "persons": [],
            "organizations": []
        }
    
    def analyze_contract_risks(self, task_id: int, contract_type: str, role: str) -> List[Dict]:
        """åˆ†æåˆåŒé£é™©"""
        db = SessionLocal()
        try:
            # è·å–æ‰€æœ‰æ®µè½
            paragraphs = db.query(Paragraph).filter(Paragraph.task_id == task_id).all()
            
            if not paragraphs:
                logger.warning(f"No paragraphs found for task {task_id}")
                return []
            
            # æ„å»ºå®Œæ•´æ–‡æœ¬
            full_text = "\n\n".join([p.text for p in paragraphs])
            
            # æ„å»ºé£é™©åˆ†ææç¤º
            prompt = self._build_risk_analysis_prompt(full_text, contract_type, role)
            
            # è°ƒç”¨LLMåˆ†æ
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åˆåŒé£é™©åˆ†æä¸“å®¶ï¼Œå…·æœ‰ä¸°å¯Œçš„æ³•å¾‹çŸ¥è¯†å’Œå®åŠ¡ç»éªŒã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            result_text = self._call_openrouter_api(messages, temperature=0.2)
            
            # è§£æé£é™©åˆ†æç»“æœ
            risks = self._parse_risk_analysis_result(result_text)
            
            # ä¿å­˜é£é™©åˆ°æ•°æ®åº“
            self._save_risks_to_db(task_id, risks, db)
            
            return risks
            
        except Exception as e:
            logger.error(f"Error analyzing contract risks: {e}")
            return []
        finally:
            db.close()
    
    def _build_risk_analysis_prompt(self, text: str, contract_type: str, role: str) -> str:
        """æ„å»ºé£é™©åˆ†ææç¤º"""
        return f"""
        è¯·å¯¹ä»¥ä¸‹{contract_type}åˆåŒè¿›è¡Œå…¨é¢çš„é£é™©åˆ†æã€‚æˆ‘çš„è§’è‰²æ˜¯{role}ã€‚
        
        åˆåŒå†…å®¹ï¼š
        {text[:4000]}  # é™åˆ¶é•¿åº¦é¿å…è¶…å‡ºtokené™åˆ¶
        
        è¯·ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œåˆ†æå¹¶ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
        
        {{
            "risks": [
                {{
                    "clause_id": "æ¡æ¬¾ç¼–å·æˆ–æ ‡è¯†",
                    "title": "é£é™©æ ‡é¢˜",
                    "risk_level": "HIGH/MEDIUM/LOW",
                    "summary": "é£é™©æè¿°å’Œåˆ†æ",
                    "suggestion": "åº”å¯¹å»ºè®®",
                    "related_laws": ["ç›¸å…³æ³•å¾‹æ³•è§„"]
                }}
            ]
        }}
        
        é‡ç‚¹å…³æ³¨ï¼š
        1. ä»˜æ¬¾æ¡æ¬¾å’Œè¿çº¦è´£ä»»
        2. äº¤ä»˜æ—¶é—´å’Œè´¨é‡æ ‡å‡†
        3. çŸ¥è¯†äº§æƒæ¡æ¬¾
        4. å…è´£å’Œé™è´£æ¡æ¬¾
        5. äº‰è®®è§£å†³æœºåˆ¶
        6. åˆåŒå˜æ›´å’Œç»ˆæ­¢æ¡ä»¶
        """
    
    def _parse_risk_analysis_result(self, result_text: str) -> List[Dict]:
        """è§£æé£é™©åˆ†æç»“æœ"""
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            result = json.loads(result_text)
            return result.get('risks', [])
        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯æ ‡å‡†JSONï¼Œå°è¯•æå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                try:
                    result = json.loads(json_match.group())
                    return result.get('risks', [])
                except json.JSONDecodeError:
                    pass
            
            logger.warning("Failed to parse risk analysis result")
            return []
    
    def _save_risks_to_db(self, task_id: int, risks: List[Dict], db: Session):
        """ä¿å­˜é£é™©åˆ°æ•°æ®åº“"""
        try:
            for risk_data in risks:
                risk = Risk(
                    task_id=task_id,
                    clause_id=risk_data.get('clause_id', ''),
                    title=risk_data.get('title', ''),
                    risk_level=risk_data.get('risk_level', 'MEDIUM'),
                    summary=risk_data.get('summary', ''),
                    suggestion=risk_data.get('suggestion', '')
                )
                db.add(risk)
                db.flush()  # è·å–risk.id
                
                # ä¿å­˜ç›¸å…³æ³•è§„
                related_laws = risk_data.get('related_laws', [])
                for law in related_laws:
                    statute = Statute(
                        risk_id=risk.id,
                        statute_ref=law,
                        statute_text=""  # å¯ä»¥åç»­è¡¥å……å…·ä½“æ¡æ–‡
                    )
                    db.add(statute)
            
            db.commit()
            logger.info(f"Saved {len(risks)} risks for task {task_id}")
            
        except Exception as e:
            db.rollback()
            logger.error(f"Error saving risks to database: {e}")
            raise

# å…¨å±€AIæœåŠ¡å®ä¾‹ - å»¶è¿Ÿåˆå§‹åŒ–
ai_service = None

def get_ai_service():
    """è·å–AIæœåŠ¡å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
    global ai_service
    if ai_service is None:
        ai_service = AIService()
    return ai_service